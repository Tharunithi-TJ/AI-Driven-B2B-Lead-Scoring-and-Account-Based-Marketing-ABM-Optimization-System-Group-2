{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Fk0y1fmoZlD"
      },
      "outputs": [],
      "source": [
        "TP, TN, FP, FN = 4, 91, 1, 4\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP, TN, FP, FN = 0, 95, 5, 0\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "4xo5O-cq3QAF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 114\n",
        "FP = 14\n",
        "precision = TP / (TP + FP)\n",
        "print(f\"precision: {precision:4.2f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WUjGCXDe37Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT (14) and TN (12) are not needed in the formuala!\n",
        "recall = TP / (TP + FN)\n",
        "print(f\"recall: {recall:4.2f}\")\n"
      ],
      "metadata": {
        "id": "FUr38ByC391w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = TP / (TP + FP)\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n"
      ],
      "metadata": {
        "id": "QtRzR83r4DXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [1, 0, 1, 0, 0, 1]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(conf_matrix)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"F1-score: \", f1)\n"
      ],
      "metadata": {
        "id": "7hzBfd3m4PwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# True labels of the data\n",
        "y_true = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
        "\n",
        "# Predicted labels of the data\n",
        "y_pred = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_true, y_pred)\n",
        "print(\"Precision: \", precision)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(\"Recall: \", recall)\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"F1 Score: \", f1)\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix: \\n\", conf_mat)\n"
      ],
      "metadata": {
        "id": "mXwxue6S4beF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "# Train the logistic regression model\n",
        "log_reg = LogisticRegression(random_state=0)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the logistic regression model\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for logistic regression\n",
        "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Precision: \", prec_log_reg)\n"
      ],
      "metadata": {
        "id": "IwPbbgu04d4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import  recall_score\n",
        "\n",
        "# Train the decision tree model\n",
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the decision tree model\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for decision tree\n",
        "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Recall: \", rec_dt)\n"
      ],
      "metadata": {
        "id": "xyxepaMc4gjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_train, y_train = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=2.3)\n",
        "\n",
        "X_test, y_test = make_blobs(n_samples=10, centers=2,\n",
        "                  random_state=0, cluster_std=4.5)\n"
      ],
      "metadata": {
        "id": "z8kiYqWu8t4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the logistic regression model\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Train the decision tree model\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the decision tree model\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for logistic regression\n",
        "acc_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "rec_log_reg = recall_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "f1_log_reg = f1_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "\n",
        "# Calculate the evaluation metrics for decision tree\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "prec_dt = precision_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Logistic Regression:\")\n",
        "print(\"Accuracy: \", acc_log_reg)\n",
        "print(\"Precision: \", prec_log_reg)\n",
        "print(\"Recall: \", rec_log_reg)\n",
        "print(\"F1-score: \", f1_log_reg)\n",
        "\n",
        "# Print the evaluation metrics for decision tree\n",
        "print(\"\\nDecision Tree:\")\n",
        "print(\"Accuracy: \", acc_dt)\n",
        "print(\"Precision: \", prec_dt)\n",
        "print(\"Recall: \", rec_dt)\n",
        "print(\"F1-score: \", f1_dt)\n"
      ],
      "metadata": {
        "id": "A1gLZwLZ8unc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}