{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6vL-Nl86Ek93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322ba6dc-af83-4489-87f9-11da40ab6c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n"
          ]
        }
      ],
      "source": [
        "#accuracy\n",
        "TP, TN, FP, FN = 4, 91, 1, 4\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "TP, TN, FP, FN = 0, 95, 5, 0\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q68PgK8yVgEH",
        "outputId": "d334fedf-a4e4-42be-f1c6-56e0cf6869d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#precision\n",
        "TP = 114\n",
        "FP = 14\n",
        "precision = TP / (TP + FP)\n",
        "print(f\"precision: {precision:4.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDeFvVWa9Di",
        "outputId": "4d553f09-1f38-4738-dbf1-f7c9f76aa233"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall\n",
        "recall = TP / (TP + FN)\n",
        "print(f\"recall: {recall:4.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9SC2WN_R3qz",
        "outputId": "ab1f10ea-632c-4065-8d1f-dfca608b81c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#F1_score\n",
        "precision = TP / (TP + FP)\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n"
      ],
      "metadata": {
        "id": "u27jGn6hR6KY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [1, 0, 1, 0, 0, 1]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(conf_matrix)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"F1-score: \", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7IRLljAaVh1",
        "outputId": "b86dbf20-54ed-4c5b-ecb2-0a2bda1221c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0]\n",
            " [1 3]]\n",
            "Accuracy:  0.8333333333333334\n",
            "Precision:  1.0\n",
            "Recall:  0.75\n",
            "F1-score:  0.8571428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# True labels of the data\n",
        "y_true = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
        "\n",
        "# Predicted labels of the data\n",
        "y_pred = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_true, y_pred)\n",
        "print(\"Precision: \", precision)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(\"Recall: \", recall)\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"F1 Score: \", f1)\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix: \\n\", conf_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGSjLk35d5Oe",
        "outputId": "bd356cfe-dec0-4704-cbdf-7a1dfe9bfdc7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.6\n",
            "Precision:  0.6\n",
            "Recall:  0.6\n",
            "F1 Score:  0.6\n",
            "Confusion Matrix: \n",
            " [[3 2]\n",
            " [2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
        "y_train = [1,2,1,2]\n",
        "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
        "y_test = [1,2,2,2]"
      ],
      "metadata": {
        "id": "76_MXtg3d-Wo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "# Train the logistic regression model\n",
        "log_reg = LogisticRegression(random_state=0)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the logistic regression model\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for logistic regression\n",
        "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Precision: \", prec_log_reg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swsL082veZsc",
        "outputId": "bf68b06b-f878-4c33-cd20-ecbd2a3e408b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
        "y_train = [1,2,1,2]\n",
        "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
        "y_test = [1,2,2,2]"
      ],
      "metadata": {
        "id": "bJoB-XbsebmW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import  recall_score\n",
        "\n",
        "# Train the decision tree model\n",
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the decision tree model\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for decision tree\n",
        "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Recall: \", rec_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoXvH6maegPu",
        "outputId": "4eff56b5-9e38-4ad4-8c2a-0d13e49af314"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall:  0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_train, y_train = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=2.3)\n",
        "\n",
        "X_test, y_test = make_blobs(n_samples=10, centers=2,\n",
        "                  random_state=0, cluster_std=4.5)"
      ],
      "metadata": {
        "id": "4mk9Wgkiegd1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the logistic regression model\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Train the decision tree model\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the decision tree model\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics for logistic regression\n",
        "acc_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "rec_log_reg = recall_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "f1_log_reg = f1_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
        "\n",
        "# Calculate the evaluation metrics for decision tree\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "prec_dt = precision_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average=\"weighted\")\n",
        "\n",
        "# Print the evaluation metrics for logistic regression\n",
        "print(\"Logistic Regression:\")\n",
        "print(\"Accuracy: \", acc_log_reg)\n",
        "print(\"Precision: \", prec_log_reg)\n",
        "print(\"Recall: \", rec_log_reg)\n",
        "print(\"F1-score: \", f1_log_reg)\n",
        "\n",
        "# Print the evaluation metrics for decision tree\n",
        "print(\"\\nDecision Tree:\")\n",
        "print(\"Accuracy: \", acc_dt)\n",
        "print(\"Precision: \", prec_dt)\n",
        "print(\"Recall: \", rec_dt)\n",
        "print(\"F1-score: \", f1_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVvR0-vEeqJ-",
        "outputId": "e6fc408e-afeb-494f-9117-25ba4a17fa19"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Accuracy:  0.7\n",
            "Precision:  0.7083333333333333\n",
            "Recall:  0.7\n",
            "F1-score:  0.696969696969697\n",
            "\n",
            "Decision Tree:\n",
            "Accuracy:  0.8\n",
            "Precision:  0.8\n",
            "Recall:  0.8\n",
            "F1-score:  0.8\n"
          ]
        }
      ]
    }
  ]
}